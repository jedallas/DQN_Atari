{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-S8EgQDzgwd4"
      },
      "source": [
        "## Configurations for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ljRXerflgwd7",
        "outputId": "d04ffd7f-126b-4418-ccb5-6bb3ae659b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as Func\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "from collections import deque\n",
        "from atari_wrappers import make_atari\n",
        "from atari_wrappers import wrap_deepmind\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "BUFFER_SIZE = int(1e4)  # replay buffer size\n",
        "BATCH_SIZE = 32         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1              # for soft update of target parameters\n",
        "LR = 1e-4               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network\n",
        "UPDATE_FREN=1000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XxteX99pgweC"
      },
      "source": [
        "## Replay buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LXUfP3o4gweC",
        "colab": {}
      },
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "  \n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e=[state, action, reward, next_state, done]\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, self.batch_size)\n",
        "        \n",
        "        \n",
        "        #(32, 4, 84, 84)\n",
        "\n",
        "        states = torch.from_numpy(np.stack([e[0] for e in experiences if e is not None])).float().to(device)\n",
        "        #feed the numpy array into training process tensor\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(device)\n",
        "\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(device)\n",
        "\n",
        "        next_states = torch.from_numpy(np.stack([e[3] for e in experiences if e is not None])).float().to(device)\n",
        "\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).int().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "    def len_Buffer(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3LP2TYY7gweE"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pfa31T_BgweF",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(state_size[1], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        conv_out = self.conv(state).view(state.size()[0], -1)      \n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NL1H4tsTwR0a",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, state_size, action_size):\n",
        "        #state_size=(32, 4, 84, 84)\n",
        "        #action_size=4\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.update_step = 0\n",
        "        self.update_fren=0\n",
        "        self.memory = Buffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "        \n",
        "\n",
        "\n",
        "        self.qnetwork_original=Network(state_size, action_size).to(device) \n",
        "        self.optimizer = torch.optim.RMSprop(self.qnetwork_original.parameters(), lr=LR)\n",
        "        self.qnetwork_target=Network(state_size, action_size).to(device) \n",
        "        \n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \n",
        "        self.memory.add(state, action, reward, next_state, done) \n",
        "        #into queue\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps. UPDATE_EVERY=4\n",
        "        self.update_step = (self.update_step + 1) % UPDATE_EVERY\n",
        "        \n",
        "        if self.update_step == 0:\n",
        "            if self.memory.len_Buffer()>=BUFFER_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                #torch from numpy\n",
        "                self.learn(experiences)\n",
        "              \n",
        "\n",
        "    def act(self, state, eps=0.0):\n",
        "      \n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        #since the state from gym is a kind of numpy\n",
        "        # (1,4,84,84)\n",
        "        # to be the same form of input for (32,4,84,84)\n",
        "        \n",
        "        self.qnetwork_original.eval()\n",
        "        #evaluation mode\n",
        "        with torch.no_grad():\n",
        "        #diabled gradient computation\n",
        "          action_values = self.qnetwork_original(state)\n",
        "\n",
        "        self.qnetwork_original.train()\n",
        "        #training mode\n",
        "\n",
        "        if random.random() >= eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        #(32,1)\n",
        "\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        #detach() Returns a new Variable, detached from the current graph\n",
        "        #max(1) maximal\n",
        "        #max(1)[0] without indices\n",
        "        #unsequeeze to be like qnetwork_original\n",
        "        #print(self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1).shape)\n",
        "        #print(self.qnetwork_target(next_states).detach().max(1)[0].shape)\n",
        "        \n",
        "        temp=GAMMA * Q_targets_next * (1 - dones)\n",
        "        Q_targets = rewards + temp\n",
        "        #print(Q_targets.shape)\n",
        "\n",
        "        Q_expected = self.qnetwork_original(states).gather(1, actions)  \n",
        "        #gather fixed at 1 axis and at 2\n",
        "        #which is equal to loc in pandas\n",
        "        #print(self.qnetwork_original(states).shape)\n",
        "        \n",
        "        #\n",
        "        loss = Func.mse_loss(Q_expected, Q_targets)\n",
        "      \n",
        "        #Minimize\n",
        "        #zero the parameter gradients \n",
        "        #initialization for each batch gradient descent\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.update_fren = (self.update_fren + 1) % UPDATE_FREN\n",
        "        \n",
        "        if self.update_fren == 0:\n",
        "            self.soft_update(self.qnetwork_original, self.qnetwork_target, TAU)    \n",
        "            #print(\"update\")                \n",
        "\n",
        "    def soft_update(self, original_model, target_model, tau):\n",
        "        #soft update\n",
        "        #zip(target_model.parameters(), original_model.parameters()) zip for original_model and target_model\n",
        "        for target_param, original_model in zip(target_model.parameters(), original_model.parameters()):\n",
        "            target_param.data.copy_(tau*original_model.data + (1.0-tau)*target_param.data)\n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9gOGqYg2dhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap_atari_dqn(env):\n",
        "    return wrap_deepmind(env, frame_stack=True, scale=True, episode_life=True,clip_rewards=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JfiGBVTQr9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap_atari_dqn_eva(env):\n",
        "    return wrap_deepmind(env, frame_stack=True, scale=True, episode_life=True,clip_rewards=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMUmLsvdRInc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jQaIPzL2dhF",
        "colab_type": "code",
        "outputId": "3f69bd6b-48cd-4e58-fa4c-ca7c1e552a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = random.randint(1,100)\n",
        "env = make_atari('BreakoutNoFrameskip-v4')\n",
        "env = wrap_atari_dqn(env)\n",
        "\n",
        "def seed_torch(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.backends.cudnn.enabled:\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "np.random.seed(seed)\n",
        "seed_torch(seed)\n",
        "env.seed(seed)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13, 460929631]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dvBFkM4fgweK"
      },
      "source": [
        "## Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6Rh2Fr-y_qI",
        "colab": {}
      },
      "source": [
        "def NMSL(timesteps=2000000,eps_start=1.0, eps_end=0.1):\n",
        "    scores = []  # list containing scores from each episode\n",
        "    scores_30=[]\n",
        "    exporation=[]\n",
        "    scores_window = deque(maxlen=30)  # last 30 scores\n",
        "    \n",
        "    eps = eps_start  # initialize epsilon\n",
        "\n",
        "    obs = env.reset()\n",
        "    state=np.transpose(np.array(obs), (2, 0, 1)) #4,84,84\n",
        "    score = 0\n",
        "\n",
        "    for timestep in range(1, timesteps+1):\n",
        "        \n",
        "        action = agent.act(state, eps)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state=np.transpose(np.array(next_state), (2, 0, 1))\n",
        "            # last three frames and current frame as the next state\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        score += reward\n",
        "     \n",
        "        eps=eps_start-(eps_start-eps_end)*timestep/1000000\n",
        "        eps = max(eps_end, eps)  # decrease epsilon     \n",
        "        \n",
        "        \n",
        "        if done:\n",
        "          scores_window.append(score) \n",
        "          scores.append(score)\n",
        "          scores_30.append(np.mean(scores_window))\n",
        "          score = 0\n",
        "          obs = env.reset()\n",
        "          state=np.transpose(np.array(obs), (2, 0, 1))\n",
        "\n",
        "    \n",
        "        if timestep % 10000 == 0:\n",
        "            print('timestep now : {:.2f}'.format(eps))\n",
        "            print('timestep {} \\tAverage Score: {:.2f}'.format(timestep, np.mean(scores_window)))\n",
        "            print('timestep {} \\tThe length of replay buffer now: {}'.format(timestep, agent.memory.len_Buffer()))\n",
        "            print('timestep {} \\tMax score now: {}'.format(timestep, np.max(scores_30)))\n",
        "            evaluation_score=0\n",
        "            enva = make_atari('BreakoutNoFrameskip-v4')\n",
        "            enva = wrap_atari_dqn_eva(enva)\n",
        "            for ep in range(30):\n",
        "              for i in range(5):\n",
        "                  obs = enva.reset()\n",
        "                  state=np.transpose(np.array(obs), (2, 0, 1)) #4,84,84\n",
        "                  for j in range(40000):\n",
        "                      action = agent.act(state, 0.001)\n",
        "                      next_state, reward, done, _ = enva.step(action)\n",
        "                      next_state=np.transpose(np.array(next_state), (2, 0, 1))\n",
        "                      state = next_state\n",
        "                      evaluation_score += reward\n",
        "                      if done:\n",
        "                          break\n",
        "            print('timestep {} \\tEvaluation score now: {}'.format(timestep, evaluation_score/30))\n",
        "            if evaluation_score/30>=300:\n",
        "              name='dqn_checkpoint_{}.pth'.format(evaluation_score/30)\n",
        "              torch.save(agent.qnetwork_original.state_dict(), name)\n",
        "\n",
        "\n",
        "       \n",
        "    torch.save(agent.qnetwork_original.state_dict(), 'dqn_checkpoint_final.pth')\n",
        "    return scores,scores_30\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfK9XoY12dhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot1(episode_rewards):\n",
        "    fig1 = plt.figure('fig1')\n",
        "    plt.plot(np.arange(len(episode_rewards)), episode_rewards)\n",
        "    plt.ylabel('Score')\n",
        "    plt.xlabel('Episode')\n",
        "    fig1.savefig('episode_rewards.jpg')\n",
        "def plot2(mean_30ep_rewards):\n",
        "    fig2 = plt.figure('fig2')\n",
        "    plt.plot(np.arange(len(mean_30ep_rewards)), mean_30ep_rewards)\n",
        "    plt.ylabel('Scores')\n",
        "    plt.xlabel('Per 30 Episode')\n",
        "    fig2.savefig('mean_30ep_rewards.jpg')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-y8oa57WgweT"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f6gsMzxpgweT",
        "colab": {}
      },
      "source": [
        "agent = Agent((32, 4, 84, 84), 4)  # state size (batch_size, 4 frames, img_height, img_width)\n",
        "TRAIN = True  # train or test flag\n",
        "\n",
        "\n",
        "if TRAIN:\n",
        "        \n",
        "        scores,scores_30 = NMSL()\n",
        "      \n",
        "        plot1(scores)\n",
        "        plot2(scores_30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EMljxbklb6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap_atari_dqn_eva(env):\n",
        "    return wrap_deepmind(env, frame_stack=True, scale=True, episode_life=False,clip_rewards=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gxy1xp8Aiz8x",
        "colab": {}
      },
      "source": [
        "agent.qnetwork_original.load_state_dict(torch.load('/content/drive/My Drive/dqn/dqn_checkpoint_6708.0.pth'))\n",
        "enva = make_atari('BreakoutNoFrameskip-v4')\n",
        "enva = wrap_atari_dqn_eva(enva)\n",
        "enva=gym.wrappers.Monitor(enva,'/content/drive/My Drive/dqn/video',force=True)\n",
        "\n",
        "\n",
        "\n",
        "for ep in range(5):\n",
        "  score=0\n",
        "  obs = enva.reset()\n",
        "  state=np.transpose(np.array(obs), (2, 0, 1)) #4,84,84\n",
        "  for j in range(100000):\n",
        "      action = agent.act(state, 0.001)\n",
        "      next_state, reward, done, _ = enva.step(action)\n",
        "      next_state=np.transpose(np.array(next_state), (2, 0, 1))\n",
        "      state = next_state\n",
        "      score += reward\n",
        "      if done:\n",
        "          print(score)\n",
        "          break\n",
        "              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f__H3QpplGTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}